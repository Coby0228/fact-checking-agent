import os
import json
import tempfile
from pathlib import Path
from tools.client import SerperClient

LOG_PATH = "serp_logs/claim.json"
serper_client = SerperClient()

def load_log():
    if os.path.exists(LOG_PATH):
        with open(LOG_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

def save_log(data):
    os.makedirs(os.path.dirname(LOG_PATH), exist_ok=True)
    with open(LOG_PATH, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)

def search_web(query: str, num_results: int = 5, claim: str = None) -> list:

    print(f"ğŸ” æœå°‹: {query} (è¿”å›æ•¸é‡: {num_results})")
    result = serper_client.run(query, num_results)
    if not result:
        print("âš ï¸ ç„¡æœå°‹çµæœ")
        return []

    logs = load_log()

    # å°‹æ‰¾ claim entry
    for entry in logs:
        if entry["claim"] == claim:
            break
    else:
        entry = {"claim": claim, "evidence": []}
        logs.append(entry)

    # å»é‡ urlï¼ˆå·²å­˜åœ¨çš„ï¼‰
    existing_urls = {e["url"] for e in entry["evidence"]}

    new_evidence = [
        item for item in result
        if item["url"] not in existing_urls
    ]

    entry["evidence"].extend(new_evidence)
    save_log(logs)

    return new_evidence

if __name__ == "__main__":

    claim = "OpenAI æ˜¯ç”±èª°å‰µè¾¦çš„ï¼Ÿ"

    search_web("OpenAI company founders", num_results=3, claim=claim)
    search_web("Who started OpenAI", num_results=3, claim=claim)

    print(f"âœ… æ¸¬è©¦å®Œæˆï¼Œè«‹æŸ¥çœ‹ {LOG_PATH}")
